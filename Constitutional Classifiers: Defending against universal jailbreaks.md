标题：Constitutional Classifiers: Defending against universal jailbreaks
标题：宪法分类：抵御普遍越狱

来源：https://www.anthropic.com/research/constitutional-classifiers?utm_source=chatgpt.com

youtube：https://www.youtube.com/watch?v=BaNXYqcfDyo

正文：
人类安全保障研究团队发表的一篇新论文描述了一种防御人工智能模型免受通用越狱攻击的方法。该方法的原型版本在数千小时的人工红队演练中表现出了鲁棒性，尽管其拒绝率较高且计算开销较大。更新后的版本在合成评估中实现了类似的鲁棒性，拒绝率仅提高了0.38%，计算成本也仅略有增加。

大型语言模型经过广泛的安全训练，以防止产生有害输出。例如，我们训练 Claude 拒绝回答用户提出的涉及生物或化学武器生产的查询。

然而，模型仍然容易受到越狱攻击：攻击者会设计输入绕过模型的安全防护措施，迫使其产生有害响应。一些越狱攻击会向模型发送过长的提示信息；另一些则会修改输入格式，例如使用非标准大写字母。历史上，越狱攻击一直难以检测和阻止：这类攻击早在十多年前就已被描述，但据我们所知，目前生产环境中仍然没有完全稳健的深度学习模型。

我们正在开发更完善的越狱防御机制，以便未来能够安全地部署功能日益强大的模型。根据我们的“负责任扩展策略”，只要我们能够通过适当的安全措施将风险降低到可接受的水平，我们就可以部署此类模型——但越狱攻击允许用户绕过这些安全措施。我们尤其希望，由宪法分类器保护的系统能够帮助我们降低已达到我们“负责任扩展政策”¹中概述的 CBRN 能力阈值的模型的越狱风险。

在我们的新论文中，我们描述了一个基于宪法分类器的系统，该系统可以保护模型免受越狱攻击。这些宪法分类器是基于合成生成数据训练的输入和输出分类器，能够以最小的过度拒绝率过滤掉绝大多数的越狱攻击，且不会产生大量的计算开销。

人工红队测试结果

我们进行了两类主要测试来评估宪法分类器的有效性。

首先，我们开发了一个系统原型，用于识别和阻止与化学、生物、放射性和核危害相关的特定科学知识。然后，我们邀请了独立的越狱者参与漏洞赏金计划，挑战他们对系统进行“红队”测试（即在实验条件下尝试破解系统，以测试其鲁棒性）。

具体来说，他们拿到了一份包含十个“禁止”查询的列表，任务是使用任何他们想要的越狱技术，让由原型宪法分类器保护的现有模型之一（在本例中为 Claude 3.5 Sonnet，2024 年 6 月版）回答所有查询。只有当模型能够对所有查询给出详细答案时，我们才认为这是一次成功的“通用”越狱。

在为期两个月的实验期间，183 名 active2 参与者花费了估计超过 3000 小时尝试越狱该模型。如果他们发现通用越狱方法，将获得最高 15000 美元的奖励。尽管付出了巨大的努力，但没有一位参与者能够通过一次越狱就迫使模型回答所有十个禁止查询——也就是说，没有发现通用越狱方法。

尽管该原型系统具有一定的抗越狱能力，但它也存在一些问题：它拒绝了太多无害的查询，并且运行需要消耗大量的计算资源。存在这两个问题的系统可能更安全，但部署起来可能并不实际。此后，我们改进了系统，在大幅减少这些问题的同时，实现了类似的稳健性。

自动化评估结果

我们进行了一系列自动化评估，合成生成了 10,000 个越狱提示，其中包括许多针对当前 LLM 的最有效攻击，以及旨在绕过分类器安全措施的攻击。我们分别在受宪法分类器保护的 Claude 3.5 Sonnet 版本（2024 年 10 月）和未受分类器保护的 Claude 版本上测试了这些提示。

在基准条件下（未启用任何防御性分类器），越狱成功率为 86%——也就是说，Claude 本身仅拦截了其中 14% 的高级越狱尝试。然而，使用宪法分类器保护克劳德却取得了显著的进步：越狱成功率降低至 4.4%，这意味着超过 95% 的越狱尝试都被拒绝了。

<img width="962" height="614" alt="image" src="https://github.com/user-attachments/assets/1cc7ff62-4044-448a-b0a3-05898e556b9d" />

同时，我们的宪法分类器并未显著增加对无害用户查询的过度拒绝率：使用分类器后，Claude 的拒绝率略有上升（0.38%），但在 5000 次对话的随机样本中，这一差异并不具有统计学意义。此外，计算成本略高于未加防护的模型（高出 23.7%）。我们正在不断改进技术，以进一步降低拒绝率和计算成本。

总体而言，我们的自动化分析发现，更新后的宪法分类器系统显著提高了 AI 模型抵御越狱的能力，且仅需极少的额外成本。

工作原理

宪法分类器的工作原理与宪法 AI 类似，后者是我们用于调整 Claude 的另一种技术。这两种技术都使用宪法：一份模型应遵循的原则清单。对于宪法分类器而言，其原则定义了允许和禁止的内容类别（例如，芥末的配方是允许的，但芥末毒气的配方则不允许）。

在 Claude 的帮助下，我们利用这一宪法生成了大量涵盖所有内容类别的合成提示和合成模型补全。我们对这些提示和补全进行扩展，以确保其多样性：这包括将它们翻译成不同的语言，并使其符合已知越狱的写作风格。

<img width="949" height="608" alt="image" src="https://github.com/user-attachments/assets/c48da268-40d8-4c08-b555-417e77b851a7" />

然后，我们使用这些合成数据来训练输入和输出分类器，以便根据给定的宪法标记（并屏蔽）潜在的有害内容。为了尽量减少过度拒绝（即无害内容被错误地标记为有害内容），我们还使用承包商生成的一组固定良性查询来训练分类器。

局限性
宪法分类器可能无法阻止所有通用越狱，但我们相信，即使是少数能够绕过我们分类器的越狱，在启用安全措施的情况下也需要付出更大的努力才能发现。未来也可能出现针对该系统的新型越狱技术；因此，我们建议使用补充防御措施。尽管如此，用于训练分类器的宪法可以快速调整，以应对新发现的攻击。

完整论文包含了关于宪法分类器方法以及分类器本身的全部细节。

宪法分类器在线演示

想亲自尝试对 Claude 进行红队演练吗？我们诚邀您体验我们基于宪法分类器（Constitutional-Classifiers）的保护系统演示，并尝试破解使用我们新技术保护的 Claude 3.5 Sonnet 版本。[2025 年 2 月 10 日更新：演示已完成。详情请见下文]。

虽然宪法分类器技术灵活，可应用于任何主题，但本次演示我们选择专注于与化学武器相关的查询。

鼓励用户尝试破解我们的产品具有重要的安全意义：我们希望在真实环境下对系统进行压力测试，超越论文中的测试。这使我们能够收集更多数据，并在未来将此方法部署到生产系统之前，进一步提升其稳健性。

演示将于 2025 年 2 月 3 日至 2025 年 2 月 10 日期间开放。演示包含一个反馈表单，您可以通过该表单联系我们，报告任何成功的破解结果。此外，演示还包含我们的《负责任披露政策》信息，我们要求所有参与者遵守该政策。我们将在本帖的更新中公布演示的任何成功之处和总体结果。

2025年2月13日更新：现场演示结果

本次演示的目标是测试我们系统对通用越狱的鲁棒性——即通过提示策略，从系统中获取针对我们提供的所有八个问题的详细有害信息。演示结束时，一名越狱者使用通用越狱方法，成功从系统中获取了所有八个问题的详细答案。

我们的演示引起了人工智能安全社区的广泛关注，共有339名越狱者<sup>3</sup>尝试破解我们的系统，进行了超过30万次的聊天互动，累计红队演练时间约3700小时。

主要发现

在计划的七天中，系统有五天成功抵御了越狱尝试。在此之前，没有任何参与者能够通过所有八个级别的挑战。在第六天和第七天，四名参与者成功通关。在这四名参与者中，有一人发现了我们认定的通用越狱方法。另一位参赛者找到了一种我们认为接近通用越狱的破解方法，因为它需要直接输入部分答案才能通过最后一题，并且还需要进行一些重要的修改才能通过其他一些题目。其余两位参赛者获得了全部8道题的详细答案，但我们认定他们的破解方法不符合我们竞赛指南中规定的通用标准。

最成功的越狱策略包括：

使用各种密码和编码来绕过输出分类器。

采用角色扮演场景，通常通过系统提示进行。

用无害的替代词替换有害关键词（例如，用“水”替换“索曼”[一种危险化学品]）。

实施提示注入攻击。

<img width="887" height="895" alt="image" src="https://github.com/user-attachments/assets/c80f8bd4-4a48-43e8-ab78-af0537010bf6" />

自动评分器分析

在演示过程中，我们使用了一个自动评分系统来判断系统输出是否足够详细，从而判断其是否具有危害性。为此，我们让 Claude 检查系统输出与内部 LLM（逻辑逻辑模型）在没有任何安全措施的情况下输出的重叠部分。然后，Claude 判断这两个输出之间是否共享了足够的细节。我们的自动评分系统虽然有效，但仍有一些需要改进的地方：

在审查评分系统的记录后，我们发现评分器拒绝评分所有问题中约 1% 的提交内容。问题 4 的拒绝率更高，达到 10%。

经审查，如果重新运行评分器，并添加额外的输出反混淆步骤，且使用仅包含有用信息的模型，则问题 4 中约 1% 的拒绝评分结果将能够通过。

总而言之，如果重新运行评分器，所有提交内容中有 0.26% 的结果将会不同。

在大多数情况下，用户只需重新运行评分器即可克服这一挑战。我们在分析过程中也修复了这些错误，并对通过第 7 题但未通过第 8 题的两名用户的提交内容进行了内部重新评分。我们的评分系统对于验证系统的稳健性至关重要，这些发现将有助于我们继续改进评分系统。

与真实威胁的比较

我们的演示与真实场景存在两个重要差异。这些差异使得攻击者更容易在我们的演示中找到成功的越狱方法，但对于我们更好地了解分类器的漏洞至关重要。

首先，自动评分器为参与者提供的反馈循环比恶意攻击者试图利用有害问题的信息采取行动时所获得的反馈循环更为紧密。在实践中，恶意攻击者只能通过实际尝试遵循模型响应的指示来测试其质量——这项任务比使用我们提供的评分器要困难得多，也耗时得多。

其次，在实际应用中，我们可以实施快速响应策略，从而迅速识别并解决系统漏洞。例如，我们可以持续监控流量，找出反复触发分类器的用户，并对这些用户进行更深入的调查，如果发现漏洞，则可能更新分类器。在演示中，我们完全没有更改分类器，因此用户可以找到针对特定分类器的越狱方法。

奖金
2月5日，我们宣布，首位成功通关全部八个关卡的用户将获得1万美元现金奖励，首位使用通用越狱方法通关全部八个关卡的用户将获得2万美元现金奖励。这两项奖金分别由两位不同的越狱者获得。为了表彰破解我们系统所付出的努力，我们决定向另外两位成功通关全部八个关卡但未获得原奖金的用户额外发放奖金。总计，我们将向成功通关演示版全部八个关卡的四位用户发放5.5万美元奖金。

获奖者

我们要感谢以下越狱者为破解我们的系统所付出的努力：

Altynbek Ismailov 和 Salia Asanova：第一个使用我们认为的通用越狱方法通过所有八个挑战关卡的参赛者（团队）。

Valen Tagliabue：第一个通过所有八个挑战关卡的参赛者。

Hunter Senft-Grupp：使用我们认为接近通用的越狱方法通过了所有八个挑战关卡。

Andres Aldana：通过了所有八个挑战关卡。

展望未来

这些结果为我们改进分类器提供了宝贵的见解。成功越狱策略的展示有助于我们了解潜在的漏洞和需要增强鲁棒性的领域。我们将继续分析结果，并将发现的内容融入到系统的后续迭代中。我们还将进一步努力降低系统的过度拒绝率并计算开销成本，同时保持可接受的抗越狱鲁棒性水平。

随着模型能力的不断提升，越狱鲁棒性已成为抵御化学、生物、放射性和核风险的关键安全要求。我们的演示表明，我们的分类器能够有效降低这些风险，尤其是在与其他方法结合使用时。

我们衷心感谢所有为本次演示贡献时间和专业知识的参与者。他们的努力为提升人工智能安全性提供了宝贵的数据。

