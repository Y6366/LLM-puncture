文章标题：Progress from our Frontier Red Team

来源：https://www.anthropic.com/news/strategic-warning-for-ai-risk-progress-and-insights-from-our-frontier-red-team?utm_source=chatgpt.com


来自我们前线红队的进展

在这篇文章中，我们将分享我们对前沿人工智能模型潜在国家安全风险发展轨迹的认识，以及我们对评估这些风险所面临的挑战和最佳实践的一些思考。本文的信息基于我们过去一年中针对四个模型版本开展的工作。我们的评估是，人工智能模型在关键的军民两用能力方面展现出快速发展的“早期预警”迹象：模型在网络安全方面的技能已接近甚至在某些情况下超过了本科生水平，在某些生物学领域也达到了专家级水平。然而，目前的模型尚未达到我们认为会对国家安全构成显著高风险的阈值。

人工智能在各个领域正迅速发展

尽管人工智能能力在许多领域快速提升，但值得注意的是，现实世界的风险取决于人工智能本身以外的多种因素。物理限制、专用设备、人类专业知识以及实际应用方面的挑战仍然是重大障碍，即便人工智能在需要智能和知识的任务方面不断进步。基于此，以下是我们对人工智能在关键领域能力发展所了解到的情况。

网络安全

在网络安全领域，2024 年是一个“从零到一”的转折点。在夺旗赛 (CTF) 中——这项网络安全挑战要求在受控环境中发现并利用软件漏洞——Claude仅用一年时间就从高中生的水平提升到了大学本科生的水平。
<img width="942" height="453" alt="image" src="https://github.com/user-attachments/assets/1cf00be0-6448-41ea-bb31-11d9c04a64f0" />

注：Claude是由美国人工智能公司Anthropic开发的大型语言模型家族，2023年3月15日发布初代版本，创始团队包含OpenAI前核心成员及GPT-3开发人员。该模型采用Constitutional AI训练方法，支持多模态处理、代码生成及跨模态推理，并注重安全性与伦理规范

我们相信这反映了能力的真实提升，因为我们专门定制开发了额外的挑战，以确保它们不会无意中出现在模型的训练数据中。

这种网络安全能力的提升在我们最新的模型 Claude 3.7 Sonnet 中得以延续。在 Cybench（一个使用 CTF 挑战来评估 LLM 的公开基准测试）上，Claude 3.7 Sonnet 在五次尝试内解决了大约三分之一的挑战，而去年同期我们最前沿的模型仅能解决约 5% 的挑战（见图 2）。

<img width="955" height="455" alt="image" src="https://github.com/user-attachments/assets/fd2154a3-b790-4097-834d-6ed2fe2dfa34" />

这些改进体现在网络安全任务的各个类别中。图 3 显示了不同模型世代在各种 CTF 任务中的改进情况，这些任务包括发现和利用远程服务器上不安全软件（“pwn”）、Web 应用程序（“web”）以及密码学原语和协议（“crypto”）中的漏洞。然而，Claude 的技能仍然落后于专家级人类。例如，它仍然难以对二进制可执行文件进行逆向工程以发现隐藏漏洞，也难以在网络环境中执行侦察和利用——至少在没有一些辅助的情况下是如此。

<img width="957" height="453" alt="image" src="https://github.com/user-attachments/assets/dad99879-e76d-41c4-9c13-db58357a9bc5" />

我们与卡内基梅隆大学的外部专家合作，在逼真的大型（约50台主机）网络靶场上进行了实验，测试模型发现和利用不安全软件漏洞以及感染网络并在网络中横向移动的能力。与传统的CTF训练相比，这些挑战模拟了实际网络行动的复杂性，要​​求模型能够执行侦察并协调多阶段网络攻击。目前，这些模型尚无法在这种网络环境中自主成功。但是，当配备由网络安全研究人员开发的一套软件工具后，Claude（以及其他LLM模型）能够使用简单的指令成功复现一起类似于已知的大规模个人身份信息从信用报告机构窃取的攻击。

<img width="945" height="526" alt="image" src="https://github.com/user-attachments/assets/4bf5e842-8e7b-424b-bf90-7de01af02582" />

该评估基础设施使我们能够在模型自主能力提高时发出预警，同时也有可能帮助提高人工智能在网络防御中的效用，其他实验室也在探索这条道路，并取得了令人鼓舞的成果。

生物安全

我们之前的文章重点介绍了生物安全评估，我们一直在继续这项工作。我们的模型在理解生物学方面取得了快速进展。在短短一年内，Claude 的表现就从在旨在测试实验室环境中常见故障排除场景的评估中逊于世界一流的病毒学专家，轻松超越了这一基准（见图 5）。

<img width="935" height="393" alt="image" src="https://github.com/user-attachments/assets/18941778-7cdc-4d9e-ba11-f0c3f18ccdef" />

不过，Claude在生物学方面的能力仍然参差不齐。例如，针对与湿实验室研究相关的模型技能进行的内部测试表明，我们的模型在理解生物学实验流程和操作DNA及蛋白质序列方面已接近人类专家的水平。我们最新的模型在克隆工作流程方面甚至超越了人类专家的水平。然而，这些模型在解读科学数据方面仍然逊于人类专家。

<img width="932" height="397" alt="image" src="https://github.com/user-attachments/assets/9d57a1a5-660c-4f79-aeca-be8220981ab9" />

为了评估这种不断提升但发展不均衡的生物学专业知识如何转化为生物安全风险，我们开展了与武器化相关任务的小型对照研究，并与世界一流的生物防御专家合作。在一项实验研究中，我们发现，与未使用模型的其他参与者相比，我们最新的模型确实能为新手提供一定程度的提升。然而，即使是使用模型的参与者所制定的最高分方案，仍然存在一些关键错误，这些错误在现实世界中会导致失败。

同样，专家红队演练的结论也存在分歧。一些专家认为模型在武器化某些方面的知识有所提高，而另一些专家则指出，模型规划中的关键错误数量过多，不足以确保攻击端到端执行的成功。总而言之，这项分析表明，我们的模型无法可靠地指导新手恶意行为者完成生物武器获取的关键实际步骤。尽管情况已迅速好转，但我们仍将继续大力投资监测生物安全风险并制定缓解措施，例如我们最近在体质分类器方面的工作，以便在模型性能达到更令人担忧的水平时做好准备。

<img width="947" height="526" alt="image" src="https://github.com/user-attachments/assets/e0e9ebe6-bfbe-4457-8ad0-b35bbc0adb6c" />

战略预警合作的益处：人工智能的快速、负责任发展

这项工作的一项重要益处在于，它帮助我们加快步伐，而非放慢速度。通过提前制定评估计划并承诺达到能够促使提升安全级别的能力阈值，Anthropic 前沿红队的工作增强了我们快速推进人工智能前沿发展的能力，并确保我们以负责任的方式开展工作。

这项工作——尤其是在与政府合作的情况下——能够切实提升安全性，并为政府官员提供有用的信息。通过自愿且互惠互利的协议，我们的模型已由美国人工智能安全研究所 (AISI) 和英国人工智能安全研究所 (AISI) 进行了部署前测试。AISI 的最新测试有助于我们了解 Claude 3.7 Sonnet 的国家安全相关能力，我们利用这些分析结果来确定该模型的人工智能安全级别 (ASL)。

Anthropic 还率先与美国能源部 (DOE) 下属的国家核安全管理局 (NNSA) 建立了开创性的合作关系。NNSA 正在机密环境下评估 Claude 系统，以获取与核和放射性风险相关的知识。由于核武器相关信息的特殊敏感性，该项目由政府直接进行红队演练。作为合作的一部分，Anthropic 分享了我们在其他核生化 (CBRN) 领域风险识别和缓解方法的经验，NNSA 将其应用于核领域。这种公私合作模式在监管严格的核领域取得成功，表明在其他敏感领域开展类似合作是可行的。

展望未来

我们应对前沿威胁的工作凸显了内部保障措施的重要性，例如我们的“负责任扩展政策”、包括人工智能安全/安保研究所在内的独立评估机构，以及有针对性的外部监督。展望未来，我们的目标是扩大测试规模，实现更频繁的测试，并采用自动化评估、信息收集、分析和报告机制。人工智能能力的确在飞速发展，但我们进行评估、更早、更可靠地检测潜在风险的能力也在不断提升。

我们正以紧迫的步伐推进这项工作。随着模型运用扩展思维能力的提升，一些由诸如Incalmo之类的网络安全工具包支持的抽象和规划方法可能会过时，而模型本身就能更好地胜任网络安全任务。基于本文讨论的生物学研究，我们认为我们的模型正逐渐接近达到人工智能安全等级3（AI Safety Level 3）所需的能力门槛，这促使我们加大投入，确保这些安全措施能够及时到位。我们相信，前沿人工智能实验室与政府之间更深入的合作对于改进我们在所有这些重点领域的评估和风险缓解措施至关重要。
